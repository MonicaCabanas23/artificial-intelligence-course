{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 6: Clasificaci√≥n Lineal\n",
    "\n",
    "En esta clase estudiaremos los problemas de clasificaci√≥n lineal, particularmente LDA (Linear deiscriminant Analysis) y regresi√≥n Log√≠stica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Pregunta:</b> \n",
    "    ¬øPorqu√© no es bueno utilizar una regresi√≥n para solucionar un problema de clasificaci√≥n?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Soluci√≥n:</b> \n",
    "    Cuando estudiamos regresi√≥n vimos que una de las caracter√≠sticas de la regresi√≥n lineal es que la media de los errores es cero. Esto hace que la media y la varianza de los errores se√°n independientes, lo cual es optimo en ML. Sin embargo, si utilizamos regresi√≥n en un problema de clasificaci√≥n, dado la naturaleza discreta de la(s) salida(s), la varianza en los datos de entrada va a producir que el valor medio del error y la varianza esten relacionadas, tal como se muestra en las siguientes imagenes:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"img/Regresion_Clas_1.png\" width=\"400\">\n",
    "<img src=\"img/Regresion_Clas_2.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA), aproximaci√≥n de Fisher\n",
    "\n",
    "Supongamos que tenemos un conjunto de datos $\\{(\\mathbf{x}^{(1)}, y^{(1)}), (\\mathbf{x}^{(2)}, y^{(2)}), \\ldots, (\\mathbf{x}^{(m)}, y^{(m)})\\}$, donde $\\mathbf{x}^{(i)}\\in\\mathbb{R}^n$ y $y \\in\\{0,1\\}$. Sea $\\text{p}(\\mathbf{x}|y=0)$ y $\\text{p}(\\mathbf{x}|y=1)$ las distribuciones de probabilidad para los valores de $\\mathbf{x}$ que perteneces a la clase $y = 0$ y $y=1$, respectivamente. La idea detras del LDA es tratar de encontrar la direcci√≥n de un eje, de tal forma que al proyectar $\\mathbf{x}$ sobre este se maximiza la separaci√≥n entre las medias de las distribuciones de probabilidad de las variables proyectadas, y al mismo tiempo se minimizan sus varianzas. En otras palabras lo que busca LDA es realizar una transformaci√≥n lineal de los datos de entrada, tal que se maximice su disperci√≥n entre clases, y se minimice su disperci√≥n dentro de cada clase. La siguiente figura explica este proceso de forma grafica:\n",
    "![title](img/LDA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¬øC√≥mo se plantea este problema?\n",
    "\n",
    "Para solucionar este problema debemos plantear la funci√≥n de costo.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Pregunta:</b> \n",
    "    ¬øCu√°l ser√≠a esa funci√≥n de costo?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Soluci√≥n:</b> \n",
    "    Esa funci√≥n de costo se define de la siguiente forma:\n",
    "\n",
    "sea $\\hat{y} = \\theta_0+\\boldsymbol\\theta^T\\mathbf{x}$ la salida estimada de mi modelo de clasificaci√≥n, teniendo como base esto, el valor medio de esta salida est√° dado por:\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\boldsymbol\\mu_i=\\frac{1}{n_i}\\sum_{k\\in C_i}\\mathbf{x}^{(k)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "donde $C_i$ es el conjunto de datos de entrada $\\mathbf{x}$ que pertenecen a la clase $i$, $\\mu_i$ es la media de estos elementos, y $n_i$ es el n√∫mero de muestras que pertencen a esa clase. y la varianza est√° dada por:\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\boldsymbol\\Sigma_i=\\frac{1}{(n_i-1)}\\sum_{k\\in C_i}(\\mathbf{x}^{(k)}-\\boldsymbol\\mu_k)(\\mathbf{x}^{(k)}-\\boldsymbol\\mu_k)^\\text{T}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "donde $\\text{T}$ es la transpuesta.\n",
    "\n",
    "De aqu√≠ se puede definir la media y la varianza de los valores predecidos de $y$, de forma que:\n",
    "\n",
    "sea $\\hat{y}= \\theta_0+\\boldsymbol\\theta^{\\text{T}}\\mathbf{x}$, se tiene:\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\begin{split}\n",
    "\\text{E}[\\hat{y}|\\mathbf{x}\\in C_i] & = & \\theta_0 +\\boldsymbol\\theta^{\\text{T}}\\boldsymbol\\mu_i \\\\\n",
    "\\text{Var}[\\hat{y}|\\mathbf{x}\\in C_i] &=& \\boldsymbol\\theta^{\\text{T}}\\boldsymbol\\Sigma_i\\boldsymbol\\theta\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "De all√≠ la funci√≥n de costo, asuminedo dos clases, es:\n",
    "\n",
    "$$\\hat{\\boldsymbol\\theta} := \\max\\limits_\\boldsymbol\\theta \\frac{(\\boldsymbol\\mu_0^{\\text{T}}\\boldsymbol\\theta-\\boldsymbol\\mu_1^{\\text{T}}\\boldsymbol\\theta)^2}{\\left(n_0\\boldsymbol\\theta^{\\text{T}}\\boldsymbol\\Sigma_0\\boldsymbol\\theta+n_1\\boldsymbol\\theta^{\\text{T}}\\boldsymbol\\Sigma_1\\boldsymbol\\theta\\right)} = \\frac{\\left((\\boldsymbol\\mu_0^{\\text{T}}-\\boldsymbol\\mu_1^{\\text{T}})\\boldsymbol\\theta\\right)^2}{\\boldsymbol\\theta^{\\text{T}}\\left(n_0\\boldsymbol\\Sigma_0+n_1\\boldsymbol\\Sigma_1\\right)\\boldsymbol\\theta}$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Pregunta:</b> \n",
    "    ¬øC√≥mo maximizar esa funci√≥n?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Soluci√≥n:</b> \n",
    "    sea $\\mathbf{m}^{\\text{T}}=\\boldsymbol\\mu_0^{\\text{T}}-\\boldsymbol\\mu_1^{\\text{T}}$ y $\\mathbf{S}=n_0\\boldsymbol\\Sigma_0+n_1\\boldsymbol\\Sigma_1$, el problema se reduce a:\n",
    "\n",
    "$$\\mathbf{J}(\\boldsymbol\\theta) = \\frac{\\left(\\mathbf{m}^{\\text{T}}\\boldsymbol\\theta\\right)^2}{\\boldsymbol\\theta^{\\text{T}}\\mathbf{S}\\boldsymbol\\theta}.$$\n",
    "\n",
    "C√≥mo $\\mathbf{S}$ es una matrix simetrica, entonces $\\mathbf{S}= \\mathbf{R}^\\text{T}\\mathbf{R}$, con $\\mathbf{R}$ la raiz de $\\mathbf{S}$, y sea $\\mathbf{v} = \\mathbf{R}\\boldsymbol\\theta$, se tiene:\n",
    "\n",
    "$$\\mathbf{J}(\\boldsymbol\\theta) = \\frac{\\left(\\mathbf{m}^{\\text{T}}\\mathbf{R}^{-1}\\mathbf{v}\\right)^2}{\\mathbf{v}^{\\text{T}}\\mathbf{v}}= \\frac{\\left(\\mathbf{m}^{\\text{T}}\\mathbf{R}^{-1}\\mathbf{v}\\right)^2}{||\\mathbf{v}||^2}=\\left(\\mathbf{m}^\\text{T}\\mathbf{R}^{-1}\\frac{\\mathbf{v}}{||\\mathbf{v}||}\\right)^2=\\left(\\left((\\mathbf{R}^{-1})^\\text{T}\\mathbf{m}\\right)^\\text{T}\\frac{\\mathbf{v}}{||\\mathbf{v}||}\\right)^2.$$\n",
    "\n",
    "se busca maximizar ese producto punto, el cual es m√°ximo si los dos vectores estan en la misma direcci√≥n. Propongamos que $\\mathbf{v}= a (\\mathbf{R}^{-1})^\\text{T}\\mathbf{m}$, donde $a$ es una constante, es el vector que m√°ximiza ese producto punto. Tomando de antes $\\mathbf{m} = \\boldsymbol\\mu_0-\\boldsymbol\\mu_1$, y $\\boldsymbol\\theta=\\mathbf{R}^{-1}\\mathbf{v}$ se tiene:\n",
    "\n",
    "$$ \\boldsymbol\\theta = a\\mathbf{R}^{-1}(\\mathbf{R^{-1}})^{\\text{T}}(\\boldsymbol\\mu_0-\\boldsymbol\\mu_1)=a(\\mathbf{R}^\\text{T}\\mathbf{R})^{-1}(\\boldsymbol\\mu_0-\\boldsymbol\\mu_1),$$\n",
    "\n",
    "lo cual se simplifica por:\n",
    "\n",
    "$$\\boldsymbol\\theta = a\\mathbf{S}^{-1}(\\boldsymbol\\mu_0-\\boldsymbol\\mu_1)=a\\left(n_0\\boldsymbol\\Sigma_0+n_1\\boldsymbol\\Sigma_1\\right)^{-1}(\\boldsymbol\\mu_0-\\boldsymbol\\mu_1),$$\n",
    "\n",
    "con $a$ una constante arbitraria que puede ser 1. De all√≠:\n",
    "$$\\theta_0 = \\text{E}[y-\\boldsymbol\\theta^\\text{T}\\mathbf{x}] = \\frac{1}{n}\\sum y^{(i)}-\\boldsymbol\\theta^\\text{T}\\mathbf{x}^{(i)}.$$\n",
    "\n",
    "De forma sencilla, lo que se esta haciendo es tomar el vector de la diferencia de ambas medias, y se rota por medio de la inversa de las covarianzas de los datos, i.e. donde mas varian los datos tiene el menor efecto en esta rotaci√≥n.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Nota:</b> LDA considera que los datos en cada clase estan distribuidos de forma Normal, y que las matrices de covarianza de las distribuciones son iguales. Fisher no requiere asumir est√°s cosas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA a traves de Bayes\n",
    "\n",
    "Se puede llegar a LDA a traves de Bayes, sabiendo que:\n",
    "\n",
    "$$\\text{p}(y=1|\\mathbf{x})=\\frac{\\text{p}(\\mathbf{x}|y=1)\\text{p}(y=1)}{\\text{p}(\\mathbf{x})},$$\n",
    "\n",
    "donde $\\text{p}(\\mathbf{x}) = \\text{p}(\\mathbf{x}|y=1)\\text{p}(y=1)+\\text{p}(\\mathbf{x}|y=0)\\text{p}(y=0)$. En t√©rminos de probabilidades el likelihood (similaridad) es $\\text{p}(\\mathbf{x}|y=1)$, el prior es $\\text{p}(y=1)$ y la probabilidad marginal es $\\text{p}(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresi√≥n Log√≠stica\n",
    "\n",
    "La regresi√≥n log√≠stica es un metodo de clasificaci√≥n, a pesar de que se conoce como regresi√≥n. Para la clasificaci√≥n quisieramos que los valores de salida de nuestro modelo este acotado entre cero (0 -1) y uno. Para lograr esto, asumamos que nuestro modelo tiene la forma:\n",
    "\n",
    "$$h_{\\boldsymbol\\theta}(\\mathbf{x}) = g(\\boldsymbol\\theta^{\\text{T}}\\mathbf{x}),$$\n",
    "\n",
    "donde $g(\\displaystyle\\mathbf{z})=\\frac{1}{1+e^{-\\mathbf{z}}}$ es una funci√≥n sigmoidal o log√≠stica. Por lo tanto la forma del modelo es:\n",
    "\n",
    "$$h_{\\boldsymbol\\theta}(\\mathbf{x}) = \\frac{1}{1+e^{-\\boldsymbol\\theta^{\\text{T}}\\mathbf{x}}}.$$\n",
    "\n",
    "La funci√≥n log√≠stica luce como se muestra en la siguiente figura:\n",
    "\n",
    "<img src=\"img/logit.png\" width=\"400\">\n",
    "\n",
    "El problema se reduce a encontrar los valores de $\\boldsymbol\\theta$ que garantizan que tenga una buena clasificaci√≥n. \n",
    "\n",
    "En el caso de un modelo de clasificaci√≥n para dos clases, la salida de este modelo se puede interpretar como la probabilidad de que la salida sea $y=1$ cuando la entrada es $\\mathbf{x}$, i.e. $\\text{p}(y=1|\\mathbf{x};\\boldsymbol\\theta)$. Teniendo en cuenta esto, y debido a que las probabilidades de pertenecer a una clase o la otra deben sumar uno, se tiene que $\\text{p}(y=0|\\mathbf{x};\\boldsymbol\\theta) = 1-\\text{p}(y=1|\\mathbf{x};\\boldsymbol\\theta)$.\n",
    "\n",
    "Para terminar se puede entonces definir la salida de tal forma que:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  \\hat{y} =\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if} & h_{\\boldsymbol\\theta}(\\mathbf{x}) \\geq 0.5\\\\\n",
    "      0 & \\text{if} & h_{\\boldsymbol\\theta}(\\mathbf{x}) < 0.5\n",
    "    \\end{cases}       \n",
    "\\end{equation}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limite de Desici√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Pregunta:</b> \n",
    "    Considerando el problema de forma geom√©trica, ¬øcuando la funci√≥n log√≠stica ser√° mayor a 0.5?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Soluci√≥n:</b> La salida del modelo ser√° 1, i.e. $g(\\mathbf{z}) \\geq 0.5$ cuando $\\boldsymbol\\theta^\\text{T}\\mathbf{x}\\geq 0$. Si graficamos en el espacio de caracter√≠sticas, el hyper-plano donde los ejes son las variables de entrada, la ecuaci√≥n $\\boldsymbol\\theta^{\\text{T}}\\mathbf{x} = 0$, se obtiene lo que se conoce co√≥mo el limite de desici√≥n (desici√≥n boundary), tal como muestra la siguiente figura:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"img/DesicionBoundary.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Este limite de decision separa el espacio de caracteristicas en dos regiones tal que si una entranda $\\mathbf{x}$ se encuentra en una de esta su salida ser√° $y=0$ o $y=1$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Pregunta:</b>  ¬øQu√© tipo de limite de desici√≥n forma el modelo $h_{\\boldsymbol\\theta}(\\mathbf{x}) = g([-1;1;1]^\\text{T}[1;x_1^2;x_2^2])$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Soluci√≥n:</b> El limite de desici√≥n estar√≠a dado por $[‚àí1;1;1]^\\text{T}[1;ùë•_1^2;ùë•_2^2]\\geq 0$, o en otras palabras por $x_1^2+x_2^2 \\geq 1$, como se muestra en la figura de abajo. Es decir que se pueden crear fronteras no lineales, de la misma forma que en el caso de la regresion lineal. Lo importante es que este elemnto del modelo sea lineal en los par√°metros $\\boldsymbol\\theta$, pero se puede hacer tan complicado como se desee eligiendo transformaciones no lineales de los regresores. \n",
    "</div>\n",
    "\n",
    "<img src=\"img/NonlinearBound.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Nota:</b> EL limite de desici√≥n depende del modelo, no de los datos, los datos se utilizan solo para calcular los par√°metros.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste del modelo\n",
    "\n",
    "Sea nuestro vector $\\mathbf{x}^{(i)} = [x_0^{(i)};x_1^{(i)};\\ldots;x_n^{(i)}]$, con $x_0^{(i)} = 1$, para todo $i$, y $\\mathbf{x}^{(i)} \\in \\mathbb{R}^{n+1}$ y $y\\in\\{0,1\\}$. Dado el set de entrenamiento como se encuentran los par√°metros $\\boldsymbol\\theta$.\n",
    "\n",
    "Definamos la funci√≥n de costo para nuestra regresi√≥n:\n",
    "\n",
    "$$\\mathbf{J}(\\boldsymbol\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\text{costo}(h_{\\boldsymbol\\theta}(\\mathbf{x}^{(i)}),y^{(i)}),$$\n",
    "\n",
    "donde $$\\text{costo}(h_{\\boldsymbol\\theta}(\\mathbf{x}^{(i)}),y^{(i)})= \\frac{1}{2}\\left(h_{\\boldsymbol\\theta}(\\mathbf{x}^{(i)})-y^{(i)}\\right)^2.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Pregunta:</b> ¬øCu√°l es el problema de esta funci√≥n de costo?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Soluci√≥n:</b>  esta funci√≥n de costo, debido a la forma de $h$ es una funci√≥n no convexa, por lo tanto gradiente descendiente puede que no converga al m√≠nimo global. Siempre es deseable tener, en lo posible, funciones de costo convexas, por lo tanto, en este caso, hay que redefinirla.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La nueva funci√≥n de costo para este problema es:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  \\text{costo}(h_\\boldsymbol\\theta(\\mathbf{x}),y) =\n",
    "    \\begin{cases}\n",
    "      -\\log(h_\\boldsymbol\\theta(\\mathbf{x})) & \\text{if} & y=1\\\\\n",
    "      -log(1-h_\\boldsymbol\\theta(\\mathbf{x})) & \\text{if} & y=0\n",
    "    \\end{cases}       \n",
    "\\end{equation}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Pregunta:</b> ¬øC√≥mo luce la funci√≥n de costo de la regresi√≥n logistica?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Soluci√≥n:</b>  La funci√≥n de costo luce como en la siguiente figura:\n",
    "</div>\n",
    "\n",
    "<img src=\"img/Costo_Logit.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Esta funci√≥n de costo garantiza que si mi predicci√≥n es correcta $h_\\boldsymbol\\theta(\\mathbf{x}) = y$ entonces el costo es 0, si la predicci√≥n es incorrecta, el costo tiende al $\\infty$. Adem√°s esta funci√≥n es convexa, lo cual facilita la aplicaci√≥n de gradient descent.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo en cuenta este nuevo costo, se obtiene que:\n",
    "\n",
    " $$ \\text{costo}(h_\\boldsymbol\\theta(\\mathbf{x}^{(i)}),y^{(i)}) = -y^{(i)}\\log(h_\\boldsymbol\\theta(\\mathbf{x}^{(i)}))-(1-y^{(i)})\\log(1-h_\\boldsymbol\\theta(\\mathbf{x}^{(i)})).$$\n",
    " \n",
    " De all√≠ la funci√≥n de costo para la regresi√≥n log√≠stica estar√° dada por:\n",
    " \n",
    " $$\\mathbf{J}(\\boldsymbol\\theta) = -\\frac{1}{m}\\left(\\sum_{i=1}^{m}y^{(i)}\\log(h_\\boldsymbol\\theta(\\mathbf{x}^{(i)}))+(1-y^{(i)})\\log(1-h_\\boldsymbol\\theta(\\mathbf{x}^{(i)}))\\right).$$\n",
    " \n",
    "Lo √∫nico que queda por hacer es encontrar los par√°metros $\\boldsymbol\\theta$ que minimizan esa funci√≥n de costo. Para hacer esto se utiliza gradient descent.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Pregunta:</b> ¬øQu√© necesitamos para poder desarrollar el gradient desecent?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Soluci√≥n:</b> Para implementar el algoritmo, necesitamos encontrar la derivada de la funci√≥n de costo en funci√≥n de los par√°metros.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Pregunta:</b> ¬øCuanto da la derivada de la funci√≥n de costo en funci√≥n de los par√°metros?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Soluci√≥n:</b> La derivada tiene como resultado:\n",
    "\n",
    "$$\\frac{\\partial\\mathbf{J}(\\boldsymbol\\theta)}{\\partial\\theta_j} = \\frac{1}{m}\\sum_{i=1}^m\\left(h_\\boldsymbol\\theta(\\mathbf{x}^{(i)})-y^{(i)}\\right)x_j^{(i)}$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As√≠, la regla de actualizaci√≥n de los pesos en gradient descent, par ala regresi√≥n logistica, esta dada por:\n",
    "\n",
    "$$ \\begin{equation}\n",
    "      \\theta_n := \\theta_n-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}\\left[h_\\boldsymbol\\theta(\\mathbf{x}^{(i)})-y^{(i)}\\right]x_n^{(i)}\\\\\n",
    "   \\end{equation}$$\n",
    "   \n",
    "Donde $x_0^{(i)}= 1$ para todo $i$. aunque luce identico al gradiente descendiente para regresi√≥n lineal, no lo es, ya que la funci√≥n $h_\\boldsymbol\\theta(\\mathbf{x})$ es diferente a la funci√≥n de regresi√≥n lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Nota 1:</b> La normalizaci√≥n de caracteristicas (regresores) tambi√©n aplica para gradeinte descendiente cuando se usa la regresi√≥n logistica. De la misma forma que aplica en regresion lineal.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Nota 2:</b> Aparte de gradient descent existen otros algoritmos m√°s sofisticados que permiten encontrar los par√°metros que minimizan la funci√≥n de costo. Generalmente ellos requieren una funci√≥n para el c√°lculo del costo y de su derivada en funci√≥n de los par√°metros. Adem√°s, ellos son m√°s r√°pidos y no requieren de sintonizar la taza de aprendizaje, pero son m√°s complicados. Estos algoritmos no se estudiaran en el curso, hacen parte del curso de Optimizaci√≥n, (Conjugate gradient, BFGS, L-BGFS)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresi√≥n Log√≠stica Multiclase\n",
    "\n",
    "Si se tienen $p$ clases, se pueden entrenar $p$ clasificadores, utilizando regresi√≥n logistica, y al final se agregan las salidas de cada clasificador, de tal forma que la clase seleccionada sea aquella que tenga la mayor probabilidad (la mayor variable latente de salida del clasificador). Tal como se explica en la figura:\n",
    "\n",
    "<img src=\"img/onevsrest.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Pregunta:</b> ¬øQu√© sucede si no escojo al final la clase que de la mayor probabilidad, sino que escojo en funci√≥n de si la probabilidad es mayor a 0.5?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Soluci√≥n:</b> si se realiza de esta forma la asignaci√≥n de la clase se puede crear una regi√≥n de ambiguedad, tal como se muestra en la figura de abajo:\n",
    "\n",
    "</div>\n",
    "<img src=\"img/Ambiguedad.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matematicamente, la salida del clasificador est√° dada por: \n",
    "\n",
    "$$\\hat{y} = \\max\\limits_ih^{(i)}_\\boldsymbol\\theta(\\mathbf{x}),$$\n",
    "\n",
    "donde $h^{(i)}_\\boldsymbol\\theta(\\mathbf{x})$ es la salida del clasificador entrenado para la clase $i$.\n",
    "\n",
    "Otra estrategia es combinar las variables latentes (antes de aplicar la funci√≥n logistica) a una funci√≥n tipo softmax, aunque esto no es generalmente conocido como regresi√≥n logistica:\n",
    "\n",
    "$$\\sigma(\\mathbf{z})_j=\\frac{e^{z_j}}{\\sum_{k=1}^pe^{z_k}},$$\n",
    "\n",
    "donde $z_k=\\boldsymbol\\theta_k^\\text{T}\\mathbf{x}$, para $k=\\{1,\\ldots,p\\}$ y $\\mathbf{z}=[z_1,\\ldots,z_p]^\\text{T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizaci√≥n en Regresi√≥n Logistica\n",
    "\n",
    "La regresi√≥n logistica puede tambien llegar a ser sobre-ajustada.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Pregunta:</b> ¬øQu√© significa que la regresi√≥n logistica pueda ser sobre-ajustada?¬øC√≥mo luce un clasificador sobreajustado?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Soluci√≥n:</b> Un clasificador sobreajustado genera limites de desici√≥n extremadamente complicados, no suaves, que no son capaces de generalizar.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar el sobreajuste en la regresi√≥n logistica, podemos usar el mismo criterio que en regresi√≥n, disminuir el n√∫mero de  regresores, o utilizar regularizaci√≥n, tipo LASSO, Tickhonov, etc... De est√° forma la nueva funci√≥n de costo ser√°:\n",
    "\n",
    " $$\\mathbf{J}(\\boldsymbol\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(h_\\boldsymbol\\theta(\\mathbf{x}^{(i)}))+(1-y^{(i)})\\log(1-h_\\boldsymbol\\theta(\\mathbf{x}^{(i)}))+\\frac{\\gamma}{2m}\\sum_{j=1}^n\\theta_j^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La actualizaci√≥n en gradiente descendiente ser√≠a:\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "    \\theta_0&:=& \\theta_0-\\alpha\\left[\\frac{1}{m}\\sum_{i=1}^{m}\\left[h_\\boldsymbol\\theta(\\mathbf{x}^{(i)})-y^{(i)}\\right]x_0^{(i)}\\right]\\\\\n",
    "\\theta_j &:=& \\theta_n-\\alpha\\left[\\frac{1}{m}\\sum_{i=1}^{m}\\left[h_\\boldsymbol\\theta(\\mathbf{x}^{(i)})-y^{(i)}\\right]x_j^{(i)}+\\frac{\\gamma}{m}\\theta_j\\right]\\\\\n",
    "\\end{split}\n",
    "\\end{equation}$$\n",
    "\n",
    "con $j={1,\\ldots,n}$. Donde el modelo $h_\\boldsymbol\\theta(\\mathbf{x})$ es la funci√≥n logistica y $x_0^{(i)}=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
